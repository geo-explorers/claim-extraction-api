---
phase: 01-core-extraction-api
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - src/config/prompts/__init__.py
  - src/config/prompts/topic_extraction.py
  - src/config/prompts/claim_extraction.py
  - src/extraction/__init__.py
  - src/extraction/topic_extractor.py
  - src/extraction/claim_extractor.py
  - src/services/__init__.py
  - src/services/claim_generation.py
autonomous: true

must_haves:
  truths:
    - "TopicExtractor calls Gemini with adapted topic prompt and returns a list of topic strings"
    - "ClaimExtractor calls Gemini with adapted claim prompt and topics, returns structured claims per topic"
    - "ClaimGenerationService orchestrates topic extraction then claim extraction and transforms output to API response shape"
    - "Gemini calls retry on 429/503 with exponential backoff via tenacity"
    - "Safety settings are set to BLOCK_NONE on all harm categories"
    - "Empty topic extraction raises ExtractionError instead of proceeding silently"
  artifacts:
    - path: "src/config/prompts/topic_extraction.py"
      provides: "Topic extraction prompt adapted for generic source text"
      contains: "source text"
    - path: "src/config/prompts/claim_extraction.py"
      provides: "Claim extraction prompt with self-containment rules and attribution stripping"
      contains: "self-contained"
    - path: "src/extraction/topic_extractor.py"
      provides: "TopicExtractor class with retry-wrapped async Gemini call"
      contains: "tenacity"
    - path: "src/extraction/claim_extractor.py"
      provides: "ClaimExtractor class with retry-wrapped async Gemini call"
      contains: "tenacity"
    - path: "src/services/claim_generation.py"
      provides: "ClaimGenerationService orchestrating two-step pipeline"
      contains: "ClaimGenerationService"
  key_links:
    - from: "src/extraction/topic_extractor.py"
      to: "src/schemas/llm.py"
      via: "uses TopicResult as Gemini response_schema"
      pattern: "response_schema=TopicResult"
    - from: "src/extraction/claim_extractor.py"
      to: "src/schemas/llm.py"
      via: "uses ClaimWithTopicResult as Gemini response_schema"
      pattern: "response_schema=ClaimWithTopicResult"
    - from: "src/services/claim_generation.py"
      to: "src/extraction/topic_extractor.py"
      via: "calls topic_extractor.extract()"
      pattern: "topic_extractor\\.extract"
    - from: "src/services/claim_generation.py"
      to: "src/extraction/claim_extractor.py"
      via: "calls claim_extractor.extract()"
      pattern: "claim_extractor\\.extract"
    - from: "src/services/claim_generation.py"
      to: "src/schemas/responses.py"
      via: "returns ClaimGenerationResponse"
      pattern: "ClaimGenerationResponse"
    - from: "src/services/claim_generation.py"
      to: "src/exceptions.py"
      via: "raises ExtractionError on empty topics"
      pattern: "ExtractionError"
---

<objective>
Build the two-step Gemini extraction pipeline: topic extraction from source text, then claim extraction mapped to those topics. Includes adapted prompts, retry logic, safety settings, and the orchestration service.

Purpose: This is the core business logic of the entire project -- the LLM-powered claim extraction that transforms raw text into structured, topic-organized claims.
Output: Working extraction pipeline ready to be wired into the API endpoint (Plan 03).
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-core-extraction-api/01-RESEARCH.md
@.planning/phases/01-core-extraction-api/01-01-SUMMARY.md
@submodules/extraction-api/src/config/prompts/topics_of_discussion_extraction_prompt.py
@submodules/extraction-api/src/config/prompts/claim_extraction_prompt.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Adapt extraction prompts for generic source text</name>
  <files>
    src/config/prompts/__init__.py
    src/config/prompts/topic_extraction.py
    src/config/prompts/claim_extraction.py
  </files>
  <action>
Adapt the extraction-api prompts (in `submodules/extraction-api/src/config/prompts/`) for generic source text. This is the most critical task -- prompt quality directly determines extraction quality.

**src/config/prompts/topic_extraction.py** -- Adapt from `topics_of_discussion_extraction_prompt.py`:

Define `TOPIC_EXTRACTION_PROMPT: str` as a module-level constant. Changes from the original:
1. Replace ALL "podcast transcript" / "episode" / "host" / "guest" with "source text"
2. Remove "title" and "description" parameters entirely -- generic text does not have these. The prompt should only accept source text.
3. Remove "Exclude segments like promotions, guest introductions" -- not relevant for articles
4. Scale topic count guidance to text length: "Extract 2-5 topics for short texts (under 1000 words), 4-8 topics for medium texts (1000-5000 words), 6-12 topics for long texts (over 5000 words)"
5. Update the example to be a generic article example, not a podcast episode
6. Keep: chronological ordering, topic merging for adjacent segments, requirement for multiple claims per topic, concise labels (3-10 words), specificity guidance
7. The prompt should have a single `{source_text}` placeholder for string formatting

**src/config/prompts/claim_extraction.py** -- Adapt from `claim_extraction_prompt.py`:

Define `CLAIM_EXTRACTION_PROMPT: str` as a module-level constant. Changes from the original:
1. Replace ALL "podcast transcript" / "episode" / "transcript" with "source text"
2. REMOVE the entire "STEP 1: ADVERTISEMENT & PROMOTION FILTERING" section -- news articles, research papers, etc. do not have ads embedded in them. Filtering legitimate URLs/calls-to-action from news articles would destroy valid content.
3. Keep "STEP 2: TOPIC ITERATION & CLAIM SET ASSIGNMENT" (renumber to STEP 1)
4. Keep "STEP 3: FACT EXTRACTION & REFINEMENT" (renumber to STEP 2) in full -- the Shuffle Rule, pronoun replacement, attribution stripping, temporal accuracy, contextual completeness, verifiability, 5-32 word constraint are ALL domain-agnostic and critical for quality
5. Keep the "CONTENT VALIDATION CHECKLIST" section
6. Update OUTPUT FORMAT to match the `ClaimWithTopicResult` schema shape: `{"claim_topics": [{"topic": "...", "claims": ["...", "..."]}]}`
7. The prompt should have `{topics}` and `{source_text}` placeholders for string formatting

**src/config/prompts/__init__.py** -- Empty file.

Quality check: After writing, re-read both prompts and confirm:
- No instance of "podcast", "transcript", "episode", "host", "guest" remains
- No ad-filtering section in claim prompt
- Self-containment rules (Shuffle Rule, pronoun replacement, attribution stripping) are preserved
- Topic count guidance scales with text length
- Placeholder variables are `{source_text}` and `{topics}` only
  </action>
  <verify>
Run `uv run python -c "from src.config.prompts.topic_extraction import TOPIC_EXTRACTION_PROMPT; from src.config.prompts.claim_extraction import CLAIM_EXTRACTION_PROMPT; assert '{source_text}' in TOPIC_EXTRACTION_PROMPT; assert '{topics}' in CLAIM_EXTRACTION_PROMPT; assert 'podcast' not in TOPIC_EXTRACTION_PROMPT.lower(); assert 'podcast' not in CLAIM_EXTRACTION_PROMPT.lower(); assert 'transcript' not in CLAIM_EXTRACTION_PROMPT.lower(); print('OK')"` to confirm prompts are clean.
  </verify>
  <done>Both prompts adapted for generic source text. No podcast-specific language remains. Self-containment and quality rules preserved. Placeholders use {source_text} and {topics}.</done>
</task>

<task type="auto">
  <name>Task 2: Build extractors with retry logic and orchestration service</name>
  <files>
    src/extraction/__init__.py
    src/extraction/topic_extractor.py
    src/extraction/claim_extractor.py
    src/services/__init__.py
    src/services/claim_generation.py
  </files>
  <action>
Build the extraction classes and the orchestration service. All must pass `ruff check` and `mypy --strict`.

**src/extraction/topic_extractor.py** -- `TopicExtractor` class:
- Constructor takes `client: genai.Client`, `model: str`, `temperature: float`
- Build `types.GenerateContentConfig` with `response_mime_type="application/json"`, `response_schema=TopicResult`, `temperature`, and `safety_settings` set to `BLOCK_NONE` on all 4 harm categories (HATE_SPEECH, HARASSMENT, SEXUALLY_EXPLICIT, DANGEROUS_CONTENT) using `types.SafetySetting` with `types.HarmCategory` and `types.HarmBlockThreshold` enums
- `async def extract(self, source_text: str) -> list[str]` method:
  - Format the topic extraction prompt with `{source_text}`
  - Call `await self.client.aio.models.generate_content(model=self.model, contents=prompt, config=self._config)`
  - Parse response: try `response.parsed` first (typed as `TopicResult | None`), fall back to `TopicResult.model_validate_json(response.text)` if `parsed` is None
  - If parsing fails entirely, raise `ExtractionError("Failed to parse topic extraction response")`
  - Check `response.candidates[0].finish_reason` -- if it indicates safety blocking (check for `SAFETY` value), raise `SafetyFilterError`
  - Return `result.topics`
- Wrap the Gemini call (NOT the whole extract method) with tenacity: `@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=30), retry=retry_if_exception_type((ResourceExhausted, ServiceUnavailable)), reraise=True)` -- import these from `google.api_core.exceptions`. If these exact exception types are not available from google-genai, check `google.genai.errors` or use broader exception types and document the choice.
- Note on tenacity + async: use the async retry pattern. Tenacity natively supports async functions decorated with `@retry`.

**src/extraction/claim_extractor.py** -- `ClaimExtractor` class:
- Same constructor pattern as TopicExtractor but uses `ClaimWithTopicResult` as `response_schema`
- `async def extract(self, source_text: str, topics: list[str]) -> list[ClaimWithTopicBaseResult]` method:
  - Format the claim extraction prompt with `{source_text}` and `{topics}` (format topics as a JSON array string)
  - Same Gemini call pattern with `response.parsed` -> fallback -> error
  - Same safety check on finish_reason
  - Return `result.claim_topics`
- Same tenacity retry decorator as TopicExtractor

**src/services/claim_generation.py** -- `ClaimGenerationService` class:
- Constructor takes `topic_extractor: TopicExtractor`, `claim_extractor: ClaimExtractor`
- `async def generate_claims(self, source_text: str) -> ClaimGenerationResponse` method:
  - Step 1: `topics = await self.topic_extractor.extract(source_text)`
  - Validate: if `not topics`, raise `EmptyExtractionError("No topics could be extracted from the source text")`
  - Step 2: `claims_by_topic = await self.claim_extractor.extract(source_text, topics)`
  - Transform LLM output to API response: flatten `ClaimWithTopicBaseResult` list into `list[ClaimResponse]` where each claim string becomes a `ClaimResponse(claim_topic=item.topic, claim=claim_text)`
  - If the flattened list is empty (topics existed but no claims extracted), raise `EmptyExtractionError("No claims could be extracted from the source text")`
  - Return `ClaimGenerationResponse(claims=claims)`
- Catch `RetryError` from tenacity (when all retries exhausted) and re-raise as `LLMProviderError` with a descriptive message including the original exception

**All `__init__.py` files** -- Empty.

Important mypy notes:
- The `google.genai` types may not have full type stubs. If mypy complains about missing stubs, add a `# type: ignore[import-untyped]` on the specific import line. Do NOT disable mypy globally.
- The `response.parsed` attribute may need a cast or type assertion. Handle it explicitly.
- All method signatures must have full return type annotations.
  </action>
  <verify>
Run all checks:
1. `uv run ruff check src/` -- 0 errors
2. `uv run mypy --strict src/` -- 0 errors (type: ignore comments acceptable on google-genai imports only)
3. `uv run python -c "from src.extraction.topic_extractor import TopicExtractor; from src.extraction.claim_extractor import ClaimExtractor; from src.services.claim_generation import ClaimGenerationService; print('OK')"` -- imports work
  </verify>
  <done>TopicExtractor and ClaimExtractor make async Gemini calls with tenacity retry on 429/503, BLOCK_NONE safety settings, and proper error propagation. ClaimGenerationService orchestrates the two-step pipeline, validates intermediate results, and transforms output to API response shape. All pass ruff and mypy.</done>
</task>

</tasks>

<verification>
1. `uv run ruff check src/` passes with 0 errors
2. `uv run mypy --strict src/` passes with 0 errors
3. All extraction classes importable and constructible (with mock client)
4. Prompts contain no podcast-specific language
5. Tenacity retry decorators present on Gemini call methods
6. Safety settings configured as BLOCK_NONE on all harm categories
7. Service raises ExtractionError on empty topics (not silent empty return)
</verification>

<success_criteria>
- Topic extraction prompt adapted for generic source text (no podcast language)
- Claim extraction prompt preserves self-containment rules, removes ad filtering
- TopicExtractor makes async Gemini call with TopicResult schema and retry logic
- ClaimExtractor makes async Gemini call with ClaimWithTopicResult schema and retry logic
- Both extractors use BLOCK_NONE safety settings on all harm categories
- ClaimGenerationService orchestrates two-step pipeline with fail-fast on empty topics
- Service transforms LLM output shape to API response shape (ClaimGenerationResponse)
- All code passes ruff check and mypy --strict
</success_criteria>

<output>
After completion, create `.planning/phases/01-core-extraction-api/01-02-SUMMARY.md`
</output>
