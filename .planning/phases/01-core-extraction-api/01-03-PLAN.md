---
phase: 01-core-extraction-api
plan: 03
type: execute
wave: 3
depends_on: ["01-01", "01-02"]
files_modified:
  - src/dependencies.py
  - src/routers/generate.py
  - src/main.py
  - tests/__init__.py
  - tests/conftest.py
  - tests/test_routers/__init__.py
  - tests/test_routers/test_health.py
  - tests/test_routers/test_generate.py
  - tests/test_services/__init__.py
  - tests/test_services/test_claim_generation.py
  - tests/test_extraction/__init__.py
  - tests/test_extraction/test_topic_extractor.py
  - tests/test_extraction/test_claim_extractor.py
autonomous: true
user_setup:
  - service: gemini
    why: "LLM provider for claim extraction"
    env_vars:
      - name: GEMINI_API_KEY
        source: "Google AI Studio (https://aistudio.google.com/apikey) -> Create API Key"

must_haves:
  truths:
    - "POST /generate/claims with valid source text returns {claims: [{claim_topic, claim}]} with 200 status"
    - "POST /generate/claims with empty/short/long text returns 422 with clear error message"
    - "POST /generate/claims surfaces Gemini errors as 502 with descriptive message"
    - "pytest passes with all tests green"
    - "ruff check, mypy --strict, and pytest all pass cleanly"
  artifacts:
    - path: "src/dependencies.py"
      provides: "FastAPI dependency injection for Settings, Gemini client, service instances"
      contains: "Depends"
    - path: "src/routers/generate.py"
      provides: "POST /generate/claims endpoint"
      contains: "/generate/claims"
    - path: "tests/conftest.py"
      provides: "Shared fixtures: mock Gemini client, sample texts, app test client"
      contains: "fixture"
    - path: "tests/test_routers/test_generate.py"
      provides: "Endpoint integration tests for /generate/claims"
      contains: "test_generate"
    - path: "tests/test_routers/test_health.py"
      provides: "Health endpoint tests"
      contains: "test_health"
    - path: "tests/test_extraction/test_topic_extractor.py"
      provides: "Unit tests for TopicExtractor with mocked Gemini"
      contains: "TopicExtractor"
    - path: "tests/test_extraction/test_claim_extractor.py"
      provides: "Unit tests for ClaimExtractor with mocked Gemini"
      contains: "ClaimExtractor"
    - path: "tests/test_services/test_claim_generation.py"
      provides: "Unit tests for ClaimGenerationService"
      contains: "ClaimGenerationService"
  key_links:
    - from: "src/routers/generate.py"
      to: "src/services/claim_generation.py"
      via: "dependency injection of ClaimGenerationService"
      pattern: "Depends.*claim_generation"
    - from: "src/routers/generate.py"
      to: "src/schemas/requests.py"
      via: "accepts ClaimGenerationRequest body"
      pattern: "ClaimGenerationRequest"
    - from: "src/routers/generate.py"
      to: "src/schemas/responses.py"
      via: "returns ClaimGenerationResponse"
      pattern: "ClaimGenerationResponse"
    - from: "src/dependencies.py"
      to: "src/config/settings.py"
      via: "creates Settings instance"
      pattern: "Settings"
    - from: "src/dependencies.py"
      to: "src/services/claim_generation.py"
      via: "constructs service with extractors"
      pattern: "ClaimGenerationService"
    - from: "src/main.py"
      to: "src/routers/generate.py"
      via: "includes generate router"
      pattern: "app\\.include_router.*generate"
---

<objective>
Wire the extraction pipeline into the FastAPI endpoint via dependency injection, add the POST /generate/claims route, and write the complete test suite covering endpoints, service logic, and extractors.

Purpose: This completes Phase 1 by making the extraction pipeline accessible via HTTP and proving correctness with tests. After this plan, the API is fully functional.
Output: Working POST /generate/claims endpoint, full test suite passing, all quality checks green.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-core-extraction-api/01-RESEARCH.md
@.planning/phases/01-core-extraction-api/01-01-SUMMARY.md
@.planning/phases/01-core-extraction-api/01-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create dependency injection and generate endpoint, wire into app</name>
  <files>
    src/dependencies.py
    src/routers/generate.py
    src/main.py
  </files>
  <action>
**src/dependencies.py** -- FastAPI dependency injection:

Create dependency functions that provide configured service instances to route handlers:

- `def get_settings() -> Settings` -- returns `Settings()` (cached with `@lru_cache` so env is read once). This replaces the module-level singleton pattern.
- `def get_gemini_client(settings: Settings = Depends(get_settings)) -> genai.Client` -- creates `genai.Client(api_key=settings.gemini_api_key)`. Also use `@lru_cache` or store on a module-level variable for singleton behavior. Note: `Depends()` does not work with `@lru_cache` directly. Instead, use the app lifespan to create the client (already done in Plan 01's `app.state.gemini_client`) and access it via `request.app.state` in the dependency. Choose the cleaner approach:
  - PREFERRED: Use `request: Request` parameter in dependency, access `request.app.state.gemini_client` (the lifespan already creates it)
  - Store settings in `app.state.settings` during lifespan too
- `def get_claim_generation_service(request: Request) -> ClaimGenerationService` -- retrieves client from `request.app.state.gemini_client`, gets settings from `request.app.state.settings`, constructs `TopicExtractor(client, settings.gemini_model, settings.gemini_temperature)`, `ClaimExtractor(client, settings.gemini_model, settings.gemini_temperature)`, and returns `ClaimGenerationService(topic_extractor, claim_extractor)`
- If constructing extractors every request is wasteful, store the service on `app.state` during lifespan instead and have the dependency just retrieve it.

**src/routers/generate.py** -- Generate endpoint:

- `router = APIRouter(tags=["generate"])`
- `POST /generate/claims` endpoint:
  - Accepts `body: ClaimGenerationRequest`
  - Depends on `service: ClaimGenerationService = Depends(get_claim_generation_service)`
  - Calls `result = await service.generate_claims(body.source_text)`
  - Returns `result` (which is already a `ClaimGenerationResponse`)
  - Response model annotation: `response_model=ClaimGenerationResponse`
  - No try/catch needed in the route handler -- exceptions from the service (ExtractionError, LLMProviderError, etc.) are HTTPExceptions and FastAPI handles them automatically. The global exception handler in main.py catches anything else.
  - Add `responses` parameter to the decorator for OpenAPI docs: `responses={422: {"model": ErrorResponse}, 502: {"model": ErrorResponse}}`

**src/main.py** -- Update to include generate router:
- Add `from src.routers.generate import router as generate_router`
- Add `app.include_router(generate_router)` after the health router include
- Update lifespan to also store `ClaimGenerationService` instance on `app.state` if using the lifespan-based approach:
  - Create `Settings()`, store on `app.state.settings`
  - Create `genai.Client(api_key=settings.gemini_api_key)`, store on `app.state.gemini_client`
  - Create `TopicExtractor(client, settings.gemini_model, settings.gemini_temperature)`
  - Create `ClaimExtractor(client, settings.gemini_model, settings.gemini_temperature)`
  - Create `ClaimGenerationService(topic_extractor, claim_extractor)`, store on `app.state.claim_generation_service`
- The dependency function then just does: `return request.app.state.claim_generation_service`
  </action>
  <verify>
1. `uv run ruff check src/` -- 0 errors
2. `uv run mypy --strict src/` -- 0 errors
3. `GEMINI_API_KEY=test-key uv run python -c "from src.main import app; print([r.path for r in app.routes])"` -- should list `/health` and `/generate/claims` routes
  </verify>
  <done>POST /generate/claims endpoint exists, wired to ClaimGenerationService via dependency injection. App lifespan creates all service instances. Route handler delegates to service and lets exceptions propagate as HTTP errors.</done>
</task>

<task type="auto">
  <name>Task 2: Write complete test suite</name>
  <files>
    tests/__init__.py
    tests/conftest.py
    tests/test_routers/__init__.py
    tests/test_routers/test_health.py
    tests/test_routers/test_generate.py
    tests/test_services/__init__.py
    tests/test_services/test_claim_generation.py
    tests/test_extraction/__init__.py
    tests/test_extraction/test_topic_extractor.py
    tests/test_extraction/test_claim_extractor.py
  </files>
  <action>
Write the full test suite. All tests mock Gemini -- NO real API calls. Tests must pass with `uv run pytest` and code must pass `mypy --strict`.

**tests/conftest.py** -- Shared fixtures:

- `sample_source_text` fixture: a realistic 200-word news article snippet (hardcoded) about a concrete topic (e.g., renewable energy policy)
- `sample_short_text` fixture: a 30-character string (below min_length)
- `sample_topics` fixture: `["Renewable Energy Investment", "Policy Changes", "Economic Impact"]`
- `sample_topic_result` fixture: `TopicResult(topics=sample_topics)`
- `sample_claims_result` fixture: a `ClaimWithTopicResult` with 2-3 `ClaimWithTopicBaseResult` items, each with 2-3 realistic claim strings following the self-containment rules
- `mock_gemini_response` factory fixture: creates a mock response object with `.parsed` attribute (set to a given Pydantic model instance) and `.text` attribute (the JSON serialization) and `.candidates` with a mock candidate having `finish_reason` of `"STOP"`. Use `unittest.mock.MagicMock` or `AsyncMock`.
- `app_client` fixture: creates a FastAPI `TestClient` (from `httpx`) with the app, overriding the `get_claim_generation_service` dependency with a mock service. Use `app.dependency_overrides`. Set `GEMINI_API_KEY=test-key` in env for Settings validation.
- `mock_genai_client` fixture: an `AsyncMock` of `genai.Client` where `client.aio.models.generate_content` is an `AsyncMock`

**tests/test_routers/test_health.py**:
- `test_health_returns_ok` -- GET /health returns 200 with `{"status": "ok"}`
- `test_health_no_auth_required` -- GET /health works without any API key (no dependency on settings)

**tests/test_routers/test_generate.py**:
- `test_generate_claims_success` -- POST /generate/claims with valid source_text returns 200 with `{"claims": [{"claim_topic": ..., "claim": ...}]}`. Mock the service to return a known ClaimGenerationResponse.
- `test_generate_claims_empty_text` -- POST with `{"source_text": ""}` returns 422
- `test_generate_claims_too_short` -- POST with `{"source_text": "short"}` returns 422
- `test_generate_claims_too_long` -- POST with `{"source_text": "x" * 50001}` returns 422
- `test_generate_claims_missing_field` -- POST with `{}` returns 422
- `test_generate_claims_extraction_error` -- Mock service to raise `ExtractionError("test")`, verify 502 response
- `test_generate_claims_llm_provider_error` -- Mock service to raise `LLMProviderError()`, verify 502 response

**tests/test_services/test_claim_generation.py**:
- `test_generate_claims_success` -- Mock both extractors to return valid data. Verify service returns ClaimGenerationResponse with correct claim count and structure.
- `test_generate_claims_empty_topics_raises` -- Mock topic extractor to return empty list. Verify `EmptyExtractionError` is raised.
- `test_generate_claims_empty_claims_raises` -- Mock topic extractor to return topics, claim extractor to return empty claims for all topics. Verify `EmptyExtractionError` is raised.
- `test_generate_claims_transforms_output_correctly` -- Verify the flattening logic: if topic extractor returns 2 topics and claim extractor returns 3 claims for topic A and 2 for topic B, the response has 5 ClaimResponse items with correct topic assignments.

**tests/test_extraction/test_topic_extractor.py**:
- `test_extract_topics_success` -- Mock `client.aio.models.generate_content` to return a response with `.parsed = TopicResult(topics=["A", "B"])`. Verify `extract()` returns `["A", "B"]`.
- `test_extract_topics_fallback_to_text_parsing` -- Mock response with `.parsed = None` but `.text = '{"topics": ["A", "B"]}'`. Verify fallback parsing works.
- `test_extract_topics_parse_failure_raises` -- Mock response with `.parsed = None` and `.text = 'invalid json'`. Verify `ExtractionError` is raised.
- `test_extract_topics_safety_filter_raises` -- Mock response where candidates finish_reason indicates safety blocking. Verify `SafetyFilterError` is raised.

**tests/test_extraction/test_claim_extractor.py**:
- `test_extract_claims_success` -- Mock Gemini to return valid ClaimWithTopicResult. Verify correct return value.
- `test_extract_claims_fallback_parsing` -- Same fallback pattern as topic extractor.
- `test_extract_claims_parse_failure_raises` -- Same error pattern.
- `test_extract_claims_includes_topics_in_prompt` -- Verify the prompt sent to Gemini includes the topic list. Check the `contents` arg passed to the mock.

All `__init__.py` files should be empty.

Important: For the TestClient, use `from fastapi.testclient import TestClient` (synchronous) since pytest-asyncio's auto mode may have issues with httpx async client. The synchronous TestClient handles async routes transparently.

For mypy compliance in tests: type annotations on fixtures and test functions, use `Any` sparingly (only for mock objects where precise typing is impractical), add `# type: ignore` comments only where absolutely necessary on mock attribute access.
  </action>
  <verify>
Run the full quality suite:
1. `uv run pytest -v` -- all tests pass
2. `uv run ruff check src/ tests/` -- 0 errors on both source and test code
3. `uv run mypy --strict src/` -- 0 errors on source code (mypy on tests is optional but preferred)
4. `uv run pytest --co -q` -- lists all test functions to confirm expected test count (approximately 16-18 tests)
  </verify>
  <done>Full test suite with ~16-18 tests covering health endpoint, generate endpoint (success + validation + errors), claim generation service (orchestration + edge cases), topic extractor (success + fallback + errors), claim extractor (success + fallback + errors). All tests mock Gemini. pytest, ruff, and mypy all pass.</done>
</task>

</tasks>

<verification>
1. `uv run ruff check src/ tests/` passes with 0 errors
2. `uv run mypy --strict src/` passes with 0 errors
3. `uv run pytest -v` passes with all tests green
4. `curl -X POST http://localhost:8000/generate/claims -H "Content-Type: application/json" -d '{"source_text": ""}'` returns 422 (manual smoke test if desired)
5. Routes exist: `/health` (GET) and `/generate/claims` (POST)
</verification>

<success_criteria>
- POST /generate/claims endpoint accepts {source_text} and returns {claims: [{claim_topic, claim}]}
- Input validation rejects empty, too-short, too-long text with 422 status
- Extraction errors surface as 502 with descriptive messages
- Dependency injection wires Settings -> Gemini client -> Extractors -> Service -> Route
- Test suite has ~16-18 tests covering all layers with mocked Gemini
- All quality gates pass: ruff check, mypy --strict, pytest
</success_criteria>

<output>
After completion, create `.planning/phases/01-core-extraction-api/01-03-SUMMARY.md`
</output>
